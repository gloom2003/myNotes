# 爬虫

## 基础知识：

1.转义：

~~~python
r".\test.csv" # 阻止字符的转义
".\\test.csv"
~~~

2.状态码418： 被服务器发现是爬虫

3.程序运行入口：

~~~python
if __name__ == "__main__":
    main()
~~~



## 1 urllib的使用

### get请求

模拟一个get请求

~~~python
import urllib.request

# get方式访问www.baidu.com，返回一个响应对象
response = urllib.request.urlopen("http://www.baidu.com")
# 读取响应对象并且设置使用解码
print(response.read().decode('utf-8'))
# response中不仅仅有服务器返回的响应对象，还包含发送请求时设置的请求头与响应头
# 例如：获取头部的相关属性的信息
print(response.getheader("Server"))
~~~

添加UA伪装：

~~~python
import urllib.request

url = "https://www.douban.com/"
headers = {
"User-Agent":
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}
# 封装一个req对象，指定访问的url、请求头
req = urllib.request.Request(url,headers=headers)
response = urllib.request.urlopen(req)
print(response.read().decode('utf-8'))
~~~

### post请求

模拟一个post请求：添加：/post并且按照post的方式提交表单即可

~~~python
import urllib.request
import urllib.parse

# bytes(...,encoding='utf-8')按照utf-8的编码转换为二进制数据。urllib.parse一个解析器，把{}里面的键值对进行编码解析
data = bytes(urllib.parse.urlencode({"hello":"world"}),encoding='utf-8')
# 发起一个post请求(添加：/post)，提交一个表单，内容为参数data
response = urllib.request.urlopen("http://httpbin.org/post",data= data)
print(response.read().decode('utf-8'))
~~~

添加UA伪装后：

~~~python
import urllib.request
import urllib.parse

url = "https://httpbin.org/post"
headers = {
"User-Agent":
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}
data = bytes(urllib.parse.urlencode({"name":"jojo"}),encoding= 'utf-8')
# 封装一个req对象，指定访问的url、请求头
req = urllib.request.Request(url,headers=headers,data=data,method="POST")
response = urllib.request.urlopen(req)
print(response.read().decode('utf-8'))
~~~



### 异常处理

~~~python
import urllib.request

try:
    response = urllib.request.urlopen("http://httpbin.org/get",timeout=0.01)
    print(response.read().decode('utf-8'))
except Exception as e:
    print("time out!"+e);
    
try:
    response = urllib.request.urlopen("http://httpbin.org/get",timeout=0.01)
    print(response.read().decode('utf-8'))
    # 全部异常？
except Exception as e:
    print("time out!"+e);
    
~~~

或者

~~~python
 	try:
        response = urllib.request.urlopen(req)
        html = response.read().decode('utf-8')
    except urllib.error.URLError as e:
        print("error!:")
        # 如果e中有code属性，则：
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
~~~



### 常用封装好的方法：

1.向传入的url发起一个get请求,返回html的内容

~~~python
import urllib.request

def getData(url):
    headers = {
        "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    # 封装一个req对象，指定访问的url、请求头
    req = urllib.request.Request(url,headers=headers)
    html = ""
    try:
        response = urllib.request.urlopen(req)
        html = response.read().decode('utf-8')
    except urllib.error.URLError as e:
        print("error!:")
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
    return html
~~~

## 2 BeatuifulSoup的使用

1. BeautifulSoup 类型：bs = BeautifulSoup(html,"html.parser")，表示整个文档
2. Tag类型 ：print(bs.button) ，存放的是标签及其内容
3. NavigableString类型：print(type(bs.h1.string)) # 类型为<class 'bs4.element.NavigableString'>,获取到的是标签里的内容（字符串）
4.    Comment类型：Comment是一个特殊Navigablestring，输出的内容不包含注释符号


~~~python
from bs4 import BeautifulSoup

# 以二进制的方式读取文件
file = open("./demo1.html","rb")
html = file.read()
# 使用html.parser解析器解析html文本
bs = BeautifulSoup(html,"html.parser")
# 获取第一个button标签,类型为Tag，存放的是标签及其内容；
print(bs.button)
# <button class="pos">
# <a href="#">返回顶部</a>
# </button>

# 获取到的是标签里的内容（字符串）
print(bs.h1.string)
print(type(bs.h1.string)) # 类型为<class 'bs4.element.NavigableString'>,NavigableString，
# 获取一个标签里的所有属性,返回一个字典
print(bs.meta.attrs) # {'charset': 'UTF-8'}

print(bs.a.string)
print(type(bs.a.string))# Comment类型
#4.Comment是一个特殊Navigablestring，输出的内容不包含注释符号
~~~

文档的遍历：

```python
bs.head.contents
```

文档的搜索：

1.find_all()方法

```python
bs.find_all(a) 表示搜索所有的a标签，存放到list中
```

配合正则表达式：

~~~python
import re
bs.find_all(re.compile(""))
~~~

kwargs 参数: 根据属性进行查询

```python
# 搜索id = "head"的标签及其子标签,返回一个list
t_list = bs.find_all(id="head")
# 搜索有class = "..."的标签及其子标签,返回一个list
t_list = bs.find_all(class_=True）
# 搜索所有class = "item"的div标签
t_list = bs.find_all('div',class_ = "item")
```

text参数：根据文本的内容进行查询

~~~python
# 查询文本中有数字的所有内容
t_list = bs.find_all(text = re.compile("\d"))
~~~

4.limit参数

```python
# 限制查询的结果数量为3条
t_list  = bs.find_all("a",limit=3)
```

5.使用css选择器来查询

```python
t_list = bs.select('title')#通过标签来查找
t_list = bs.select(".mnav")#通过类名来查找
t_List = bs.select("#u1")#通过id来查找
t_List = bs.select("a[class='bri']")#通过属性来查找
t_list = bs.select("head > title")#通过子标签来查找
t_list = bs.select(".mnav ~.bri")# 通过兄弟标签来查找
```

## 3 正则表达式 re

1.使用compile（）方法

~~~python
import re

pat = re.compile("AA")
res = pat.search("ABCAAFAA")
print(res) # <_sre.SRE_Match object; span=(3, 5), match='AA'> 匹配了第一个AA
~~~

2.使用search()与findall()方法

~~~python
import re

res = re.search("\d{2}","dj193") # 规则与待匹配的字符串，根据规则匹配字符(数量默认为1)，匹配成功则封装为一个对象返回
print(res)# <_sre.SRE_Match object; span=(2, 4), match='19'>

res1 = re.findall("\d","dj1234dd")# 根据规则匹配每一个字符，匹配成功的就存放到list中
print(res1) # ['1', '2', '3', '4']
~~~

3.sub()方法

~~~python
res2 = re.sub("a","1","abcdaat")# 在第三个字符串中找到第一个字符串并且使用第二个字符串进行替换
print(res2)# 1bcd11t
~~~



例子：

~~~python
# 正则表达式<a href="(.*?)/">中的.*?表示匹配任意字符（.），重复零次或多次（*）
# 但尽可能少地匹配（非贪婪匹配，?）。括号（）表示提取匹配的部分。
findLink = re.compile(r'<a href="(.*?)/">')
~~~

例子2：

~~~
给你两条数据："导演: 克里斯托夫·巴拉蒂 Christophe Barratier   主演: 让-巴蒂斯特·莫尼... 2004 / 法国 瑞士 德国 / 剧情 音乐","导演: 朱塞佩·托纳多雷 Giuseppe Tornatore   主演: 蒂姆·罗斯 Tim Roth / ... 1998 / 意大利 / 剧情 音乐",并且还有相同格式的数据250条，请向上面一样使用python的re模块使用正则表达式提取数据中的每个部分，如：导演、主演、年份、国家、分类
~~~

回答：

~~~python
import re

# 匹配导演、主演、年份、国家、分类
pattern = re.compile(
    # 匹配 "导演:" 文本，后面跟随任意数量的空白字符（\s*）。(?P<director>.*?) 是一个命名捕获组，它会匹配尽可能少的任意字符直到遇到下一个部分的模式，并将这部分匹配的文本命名为"director"。匹配后面的一个或多个空白字符（\s+）作为分隔符。
    r'导演:\s*(?P<director>.*?)\s+'
    r'主演:\s*(?P<cast>.*?)\s+'
    r'(?P<year>\d{4})\s*/\s*'
    r'(?P<country>.*?)\s*/\s*'
    r'(?P<genre>.*)'
)

# 示例数据列表
data_list = [
    "导演: 克里斯托夫·巴拉蒂 Christophe Barratier   主演: 让-巴蒂斯特·莫尼... 2004 / 法国 瑞士 德国 / 剧情 音乐",
    "导演: 朱塞佩·托纳多雷 Giuseppe Tornatore   主演: 蒂姆·罗斯 Tim Roth / ... 1998 / 意大利 / 剧情 音乐",
    #... 这里会是更多的数据，总共250条
]

# 对于每条数据，使用正则表达式提取信息
for data in data_list:
    # groupdict()方法返回一个字典，其中包含每个命名捕获组对应的信息。
    match = pattern.match(data)
    if match:
        info = match.groupdict()
        print(info)
    else:
        print(f"Data does not match pattern: {data}")
~~~





## 4 写入数据到Excel xlwt库

使用

~~~python
import xlwt
# 创建一个workBook对象，使用utf-8方式编码
workBook = xlwt.Workbook(encoding="utf-8")
# 创建一个sheet，命名为sheet1
sheet1 = workBook.add_sheet("sheet1")
# 往sheet1中的(0,0)位置写内容
sheet1.write(0,0,"hello world")
# Excel文档命名为my.xls,默认保存路径为当前目录下
workBook.save("my.xls")
~~~



## 5 sqlLite数据库

基本使用：

~~~python
import sqlite3

# 打开或者创建数据库文件
connect = sqlite3.connect("test.db")
# 获取游标
cursor = connect.cursor()
sql = '''
    create table company(
    id int primary key not null,
    name text not null,
    salary real)
'''
# 执行sql
cursor.execute(sql)


connect.commit()
connect.close()
print("success!")
~~~

获取查询到的数据：

~~~python
# 执行sql
cursor.execute(sql)

# 查询多行结果
rows = cursor.fetchall()
# 返回一个列表，列表里的每个元素是一个元组，查询出来的一行结果对应一个元组，每个元组存储着每个字段的信息
print(rows)
# 打印查询结果
for row in rows:
    print(row)

# 获取单个查询结果
rows = cursor.fetchone()
# 返回一个元组,存储着查询出来的一行的每个字段的信息
print(rows)
# 打印查询结果
print(rows[0])
~~~



## 案例

###  1 爬取豆瓣电影Top250并写入Excel表

~~~python
import urllib.request
from bs4 import BeautifulSoup
import re
import xlwt

# 正则表达式<a href="(.*?)/">中的.*?表示匹配任意字符（.），重复零次或多次（*）
# 但尽可能少地匹配（非贪婪匹配，?）。括号（）表示提取匹配的部分。
findLink = re.compile(r'<a href="(.*?)/">')
findImgSrc = re.compile(r'<img.*src="(.*?)"',re.S) # re.S使.能够匹配换行符
findTitle = re.compile(r'<span class="title">(.*?)</span>')
# 评分
findRating = re.compile(r'<span class="rating_num" property="v:average">(.*?)</span>')
# 评分人数
findJudge = re.compile(r'<span>(.*?)人评价</span>')
# 简介
findInq = re.compile(r'<span class="inq">(.*?)</span>')
# 相关内容
findBd = re.compile(r'<p class="">(.*?)</p>',re.S)

def savaData(firstCol,dataList,savePath):
    workBook = xlwt.Workbook(encoding="utf-8")
    sheet = workBook.add_sheet("豆瓣Top250")
    # 初始化第一行的目录
    for i in range(0,len(firstCol)):
        sheet.write(0,i,firstCol[i])
    # 写入数据主体
    for i in range(0,len(dataList)):
        for j in range(0,len(firstCol)):
            sheet.write(i+1,j,dataList[i][j])
    workBook.save(savePath)

def getData(baseUrl):
    dataList = []
    url = baseUrl
    for i in range(0,10):
        baseUrl = url + str(i*25)
        html = askUrl(baseUrl)
        bs = BeautifulSoup(html,"html.parser")
        # 使用bs找到每一个item
        items = bs.find_all('div',class_="item")
        for item in items:
            # 存储每条影片的相关数据
            data = []
            item = str(item)
            # 使用re匹配所有的信息，如：link，并且只获取第一个
            link = re.findall(findLink,item)[0]
            data.append(link)
            # 获取图片链接
            picture = re.findall(findImgSrc,item)[0]
            data.append(picture)
            # 获取标题名称
            titles = re.findall(findTitle,item)
            if(len(titles) == 2):
                # 中文标题
                cTitle = titles[0]
                data.append(cTitle)
                # 外国标题，去除无关符号
                oTitle = titles[1].replace("/","")
                data.append(oTitle)
            else:
                # 其他情况时，只添加中文标题，外国标题使用空字符串占位即可
                cTitle = titles[0]
                data.append(cTitle)
                oTitle = " "
                data.append(oTitle)
            # 获取评分、评价人数、简介、相关内容
            rating = re.findall(findRating,item)[0]
            data.append(rating)
            judge = re.findall(findJudge,item)[0]
            data.append(judge)
            # 有的inq简介是空的
            inqs = re.findall(findInq,item)
            inq = " "
            if len(inqs) != 0:
                inq = inqs[0]
            data.append(inq)
            # 处理Bd相关内容的字符串格式
            Bd = re.findall(findBd,item)[0]
            # (\s+)?：匹配任意数量（包括0）的空白字符（比如空格、制表符、换行等）
            Bd = re.sub(r'<br(\s+)?/>(\s+)?'," ",Bd) # 查找字符串Bd中所有的<br>或者<br />标签
            Bd = Bd.strip()
            data.append(Bd)
            dataList.append(data)
    return dataList

def askUrl(url):
    headers = {
        "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    # 封装一个req对象，指定访问的url、请求头
    req = urllib.request.Request(url,headers=headers)
    html = ""
    try:
        response = urllib.request.urlopen(req)
        html = response.read().decode('utf-8')
    except urllib.error.URLError as e:
        print("error!:")
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
    return html

if __name__ == "__main__":
    url = "https://movie.douban.com/top250?start="
    savePath = ".//豆瓣Top250.xls"
    # Excel第一行的信息
    firstCol = ("电影详情链接", "图片链接", "影片中文名", "影片外国名", "评分", "评价数", "概括", "相关信息")
    dataList = getData(url)
    savaData(firstCol,dataList,savePath)
    print("爬取完毕！")
~~~

### 2 爬取豆瓣电影Top250写入sqlLite数据库并进行数据分析

~~~python
# -*- coding = utf-8 -*-
# @Time : 2023/11/7
# @Software: PyCharm
import urllib.request
from bs4 import BeautifulSoup
import re
import sqlite3
from collections import Counter


# 正则表达式

findLink = re.compile(r'<a href="(.*?)/">')
findImgSrc = re.compile(r'<img.*src="(.*?)"',re.S) # re.S使.能够匹配换行符
findTitle = re.compile(r'<span class="title">(.*?)</span>')
# 评分
findRating = re.compile(r'<span class="rating_num" property="v:average">(.*?)</span>')
# 评分人数
findJudge = re.compile(r'<span>(.*?)人评价</span>')
# 简介
findInq = re.compile(r'<span class="inq">(.*?)</span>')
# 相关内容
findBd = re.compile(r'<p class="">(.*?)</p>',re.S)

# 匹配导演、主演、年份、国家、分类
pattern = re.compile(
    r'导演:\s*(?P<director>.*?)\s+'
    r'主演:\s*(?P<cast>.*?)\s+'
    r'(?P<year>\d{4})\s*/\s*'
    r'(?P<country>.*?)\s*/\s*'
    r'(?P<genre>.*)'
)

def main():
    url = "https://movie.douban.com/top250?start="
    # 保存到哪个数据库
    DbPath = "movie.db"
    dataList = getData(url)
    saveDataToDb(dataList,DbPath)
    print("爬取完毕！已存储到%s数据库"%DbPath)


def saveDataToDb(dataList, DbPath):
    connect = sqlite3.connect(DbPath)
    cursor = connect.cursor()
    for data in dataList:
        # 给文本类型添加双引号
        for i in range(0,len(data)):
            if(i==4 or i==5):
                continue
            data[i] = '"' + data[i] + '"'
        str = ",".join(data)
        sql = '''
            insert into movie250(info_link,pic_link,cname,ename,score,rated,instroduction,info) values(%s)
        '''%str
        cursor.execute(sql)
        connect.commit()
    connect.close()


def getData(baseUrl):
    dataList = []
    url = baseUrl
    for i in range(0,10):
        baseUrl = url + str(i*25)
        html = askUrl(baseUrl)
        bs = BeautifulSoup(html,"html.parser")
        # 使用bs找到每一个item
        items = bs.find_all('div',class_="item")
        for item in items:
            # 存储每条影片的相关数据
            data = []
            item = str(item)
            # 使用re匹配所有的信息，如：link，并且只获取第一个
            link = re.findall(findLink,item)[0]
            data.append(link)
            # 获取图片链接
            picture = re.findall(findImgSrc,item)[0]
            data.append(picture)
            # 获取标题名称
            titles = re.findall(findTitle,item)
            if(len(titles) == 2):
                # 中文标题
                cTitle = titles[0]
                data.append(cTitle)
                # 外国标题，去除无关符号
                oTitle = titles[1].replace("/","")
                data.append(oTitle)
            else:
                # 其他情况时，只添加中文标题，外国标题使用空字符串占位即可
                cTitle = titles[0]
                data.append(cTitle)
                oTitle = " "
                data.append(oTitle)
            # 获取评分、评价人数、简介、相关内容
            rating = re.findall(findRating,item)[0]
            data.append(rating)
            judge = re.findall(findJudge,item)[0]
            data.append(judge)
            # 有的inq简介是空的
            inqs = re.findall(findInq,item)
            inq = " "
            if len(inqs) != 0:
                inq = inqs[0]
            data.append(inq)
            # 处理Bd相关内容的字符串格式
            Bd = re.findall(findBd,item)[0]
            # (\s+)?：匹配任意数量（包括0）的空白字符（比如空格、制表符、换行等）
            Bd = re.sub(r'<br(\s+)?/>(\s+)?'," ",Bd) # 查找字符串Bd中所有的<br>或者<br />标签
            Bd = Bd.strip()
            data.append(Bd)
            dataList.append(data)
    return dataList

def askUrl(url):
    headers = {
        "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    # 封装一个req对象，指定访问的url、请求头
    req = urllib.request.Request(url,headers=headers)
    html = ""
    try:
        response = urllib.request.urlopen(req)
        html = response.read().decode('utf-8')
    except urllib.error.URLError as e:
        print("error!:")
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
    return html

def initDb(DbPath):
    connect = sqlite3.connect(DbPath)
    cursor = connect.cursor()
    sql = '''
        create table movie250(
        id integer primary key autoincrement,
        info_link text,
        pic_link text,
        cname varchar,
        ename varchar,
        score numeric,
        rated numeric,
        instroduction text,
        info text
        )
    '''
    cursor.execute(sql)
    connect.commit()
    connect.close()

def score():
    connect = sqlite3.connect("movie.db")
    cursor = connect.cursor()

    print("===================评分方面=========================")
    # 平均评分：
    sql = '''
        select avg(score) from movie250
    '''
    cursor.execute(sql)
    # 获取查询结果
    avg = cursor.fetchone()
    # 打印查询结果
    print("豆瓣Top250电影的平均评分:%.2f"%avg[0])

    # 最高评分：
    sql = '''
        select max(score),cname from movie250
    '''
    cursor.execute(sql)
    # 获取查询结果
    max = cursor.fetchone()
    # 打印查询结果
    print(f"豆瓣Top250电影的最高评分:{max[0]},影片名：{max[1]}")

    # 最低评分：
    sql = '''
            select min(score),cname from movie250
        '''
    cursor.execute(sql)
    # 获取查询结果
    min = cursor.fetchone()
    # 打印查询结果
    print(f"豆瓣Top250电影的最低评分:{min[0]},影片名：{min[1]}")
    connect.close()
    print("===================评分方面=========================")

def ratedNumber():
    print("==================评价人数方面=======================")
    connect = sqlite3.connect("movie.db")
    cursor = connect.cursor()

    # 总评价人数
    sql = '''
        select sum(rated) from movie250
    '''
    cursor.execute(sql)
    # 获取查询结果
    sum = cursor.fetchone()
    # 打印查询结果
    print(f"豆瓣Top250电影的总人数为: {sum[0]}人")

    # 平均人数：
    sql = '''
        select avg(rated) from movie250
    '''
    cursor.execute(sql)
    # 获取查询结果
    avg = cursor.fetchone()
    # 打印查询结果
    print(f"豆瓣Top250每部电影的平均评分人数为: {avg[0]:.0f}人")

    # 最多评价人数及其影片名：
    sql = '''
            select max(rated),cname,score from movie250
        '''
    cursor.execute(sql)
    # 获取查询结果
    res = cursor.fetchone()
    max = res[0]
    cname = res[1]
    score = res[2]
    # 打印查询结果
    print(f"豆瓣Top250电影中评分人数最多的为: {max:.0f}人，影片名：{cname},评分:{score:.1f}")

    # 最少评价人数及其影片名：
    sql = '''
                select min(rated),cname,score from movie250
            '''
    cursor.execute(sql)
    # 获取查询结果
    res = cursor.fetchone()
    min = res[0]
    cname = res[1]
    score = res[2]
    # 打印查询结果
    print(f"豆瓣Top250电影中评分人数最少的为: {min:.0f}人，影片名：{cname},评分:{score:.1f}")
    print("==================评价人数方面=======================")
    connect.close()


# 统计字符串列表中每个字符串出现最多的字符串与次数
def find_most_frequent_strings(strings):
    # 使用Counter来计数,计算字符串列表中每个字符串出现的次数
    counter = Counter(strings)
    # 找到出现次数最多的字符串和次数
    return counter.most_common(1)[0]

# 统计字符串列表中每个字符串出现最少的字符串与次数
def find_min_frequent_strings(strings):
    # 使用Counter来计数,计算字符串列表中每个字符串出现的次数
    counter = Counter(strings)
    # 找到出现次数最少的字符串和次数
    return counter.most_common()[-1]


def get_max_director(dicts):
    directors = []
    for dict in dicts:
        director = dict.get("director")
        # 处理多个导演的情况
        str_list = director.split("/")
        for str in str_list:
            directors.append(str)
    return find_most_frequent_strings(directors)

def get_max_cast(dicts):
    casts = []
    for dict in dicts:
        cast = dict.get("cast")
        str_list = cast.split("/")
        for str in str_list:
            # 处理格式
            if(str.endswith("...")):
                continue
            casts.append(str)
    return find_most_frequent_strings(casts)


def get_max_year(dicts):
    years = []
    for dict in dicts:
        year = dict.get("year")
        years.append(year)
    return find_most_frequent_strings(years)


def get_min_year(dicts):
    years = []
    for dict in dicts:
        year = dict.get("year")
        years.append(year)
    return find_min_frequent_strings(years)


def get_max_country(dicts):
    countrys = []
    for dict in dicts:
        country = dict.get("country")
        # 处理格式
        str_list = country.split(" ")
        for str in str_list:
            countrys.append(str)
    return find_most_frequent_strings(countrys)


def get_min_country(dicts):
    countrys = []
    for dict in dicts:
        country = dict.get("country")
        # 处理格式
        str_list = country.split(" ")
        for str in str_list:
            countrys.append(str)
    return find_min_frequent_strings(countrys)

def get_max_genre(dicts):
    genres = []
    for dict in dicts:
        genre = dict.get("genre")
        # 处理格式
        str_list = genre.split(" ")
        for str in str_list:
            genres.append(str)
    counter = Counter(genres)
    return counter.most_common()


def get_min_genre(dicts):
    genres = []
    for dict in dicts:
        genre = dict.get("genre")
        # 处理格式
        str_list = genre.split(" ")
        for str in str_list:
            genres.append(str)
    counter = Counter(genres)
    return counter.most_common()


def otherMessage():
    connect = sqlite3.connect("movie.db")
    cursor = connect.cursor()

    print("===================其他信息方面=========================")
    sql = '''
        select info from movie250
    '''
    cursor.execute(sql)
    rows = cursor.fetchall()
    datas = []
    for row in rows:
        message = row[0]
        # 使用正则表达式进行匹配
        match = pattern.match(message)
        if match:
            # 把匹配到的数据保存到字典中
            dict = match.groupdict()
            datas.append(dict)
        # else:
        #     # print(f"Data does not match pattern: {message}")
    max_director = get_max_director(datas)
    max_cast = get_max_cast(datas)
    max_year = get_max_year(datas)
    min_year = get_min_year(datas)
    max_country = get_max_country(datas)
    min_country = get_min_country(datas)
    max_genre = get_max_genre(datas)
    min_genre = get_min_genre(datas)
    print("豆瓣Top250电影中:")
    print(f'出现次数最多的导演为:{max_director[0]},出现的次数为{max_director[1]}')
    print(f'出现次数最多的主演为:{max_cast[0]},出现的次数为{max_cast[1]}')
    print(f'出现次数最多的年份为:{max_year[0]},出现的次数为{max_year[1]}')
    print(f'出现次数最少的年份为:{min_year[0]},出现的次数为{min_year[1]}')
    print(f'出现次数最多的国家为:{max_country[0]},出现的次数为{max_country[1]}')
    print(f'出现次数最少的国家为:{min_country[0]},出现的次数为{min_country[1]}')
    print(f'最热门分类的前3名与出现次数分别为:{max_genre[0][0]}:{max_genre[0][1]},'
          f'{max_genre[1][0]}:{max_genre[1][1]},{max_genre[2][0]}:{max_genre[2][1]}')
    print(f'最不热门分类的前3名与出现次数分别为:{min_genre[-1][0]}:{min_genre[-1][1]},'
          f'{min_genre[-2][0]}:{min_genre[-2][1]},{min_genre[-3][0]}:{min_genre[-3][1]}')
    print("===================其他信息方面=========================")

if __name__ == "__main__":
    # 第一次运行时，需要先创建数据库
    # initDb("movie.db")
    # main函数：爬取数据并存储到数据库中
    # main()
    # 数据分析相关的函数
    score()
    print()
    ratedNumber()
    print()
    otherMessage()
~~~

