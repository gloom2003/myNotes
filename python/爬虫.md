# 爬虫

## 基础知识：

1.转义：

~~~python
r".\test.csv" # 阻止字符的转义
".\\test.csv"
~~~

2.状态码418： 被服务器发现是爬虫

3.程序运行入口：

~~~python
if __name__ == "__main__":
    main()
~~~



## 1 urllib的使用

### get请求

模拟一个get请求

~~~python
import urllib.request

# get方式访问www.baidu.com，返回一个响应对象
response = urllib.request.urlopen("http://www.baidu.com")
# 读取响应对象并且设置使用解码
print(response.read().decode('utf-8'))
# response中不仅仅有服务器返回的响应对象，还包含发送请求时设置的请求头与响应头
# 例如：获取头部的相关属性的信息
print(response.getheader("Server"))
~~~

添加UA伪装：

~~~python
import urllib.request

url = "https://www.douban.com/"
headers = {
"User-Agent":
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}
# 封装一个req对象，指定访问的url、请求头
req = urllib.request.Request(url,headers=headers)
response = urllib.request.urlopen(req)
print(response.read().decode('utf-8'))
~~~

### post请求

模拟一个post请求：添加：/post并且按照post的方式提交表单即可

~~~python
import urllib.request
import urllib.parse

# bytes(...,encoding='utf-8')按照utf-8的编码转换为二进制数据。urllib.parse一个解析器，把{}里面的键值对进行编码解析
data = bytes(urllib.parse.urlencode({"hello":"world"}),encoding='utf-8')
# 发起一个post请求(添加：/post)，提交一个表单，内容为参数data
response = urllib.request.urlopen("http://httpbin.org/post",data= data)
print(response.read().decode('utf-8'))
~~~

添加UA伪装后：

~~~python
import urllib.request
import urllib.parse

url = "https://httpbin.org/post"
headers = {
"User-Agent":
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}
data = bytes(urllib.parse.urlencode({"name":"jojo"}),encoding= 'utf-8')
# 封装一个req对象，指定访问的url、请求头
req = urllib.request.Request(url,headers=headers,data=data,method="POST")
response = urllib.request.urlopen(req)
print(response.read().decode('utf-8'))
~~~



### 异常处理

~~~python
import urllib.request

try:
    response = urllib.request.urlopen("http://httpbin.org/get",timeout=0.01)
    print(response.read().decode('utf-8'))
except Exception as e:
    print("time out!"+e);
    
try:
    response = urllib.request.urlopen("http://httpbin.org/get",timeout=0.01)
    print(response.read().decode('utf-8'))
    # 全部异常？
except Exception as e:
    print("time out!"+e);
    
~~~

11

~~~python
 	try:
        response = urllib.request.urlopen(req)
        html = response.read().decode('utf-8')
    except urllib.error.URLError as e:
        print("error!:")
        # 如果e中有code属性，则：
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
~~~



### 常用封装好的方法：

~~~python
import urllib.request

def getData(url):
    headers = {
        "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    # 封装一个req对象，指定访问的url、请求头
    req = urllib.request.Request(url,headers=headers)
    html = ""
    try:
        response = urllib.request.urlopen(req)
        html = response.read().decode('utf-8')
    except urllib.error.URLError as e:
        print("error!:")
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
    return html
~~~

## 2 BeatuifulSoup的使用

1. BeautifulSoup 类型：bs = BeautifulSoup(html,"html.parser")，表示整个文档
2. Tag类型 ：print(bs.button) ，存放的是标签及其内容
3. NavigableString类型：print(type(bs.h1.string)) # 类型为<class 'bs4.element.NavigableString'>,获取到的是标签里的内容（字符串）
4.    Comment类型：Comment是一个特殊Navigablestring，输出的内容不包含注释符号


~~~python
from bs4 import BeautifulSoup

# 以二进制的方式读取文件
file = open("./demo1.html","rb")
html = file.read()
# 使用html.parser解析器解析html文本
bs = BeautifulSoup(html,"html.parser")
# 获取第一个button标签,类型为Tag，存放的是标签及其内容；
print(bs.button)
# <button class="pos">
# <a href="#">返回顶部</a>
# </button>

# 获取到的是标签里的内容（字符串）
print(bs.h1.string)
print(type(bs.h1.string)) # 类型为<class 'bs4.element.NavigableString'>,NavigableString，
# 获取一个标签里的所有属性,返回一个字典
print(bs.meta.attrs) # {'charset': 'UTF-8'}

print(bs.a.string)
print(type(bs.a.string))# Comment类型
#4.Comment是一个特殊Navigablestring，输出的内容不包含注释符号
~~~

文档的遍历：

```python
bs.head.contents
```

文档的搜索：

1.find_all()方法

```python
bs.find_all(a) 表示搜索所有的a标签，存放到list中
```

配合正则表达式：

~~~python
import re
bs.find_all(re.compile(""))
~~~

kwargs 参数: 根据属性进行查询

```python
# 搜索id = "head"的标签及其子标签,返回一个list
t_list = bs.find_all(id="head")
# 搜索有class = "..."的标签及其子标签,返回一个list
t_list = bs.find_all(class_=True）
# 搜索所有class = "item"的div标签
t_list = bs.find_all('div',class_ = "item")
```

text参数：根据文本的内容进行查询

~~~python
# 查询文本中有数字的所有内容
t_list = bs.find_all(text = re.compile("\d"))
~~~

4.limit参数

```python
# 限制查询的结果数量为3条
t_list  = bs.find_all("a",limit=3)
```

5.使用css选择器来查询

```python
t_list = bs.select('title')#通过标签来查找
t_list = bs.select(".mnav")#通过类名来查找
t_List = bs.select("#u1")#通过id来查找
t_List = bs.select("a[class='bri']")#通过属性来查找
t_list = bs.select("head > title")#通过子标签来查找
t_list = bs.select(".mnav ~.bri")# 通过兄弟标签来查找
```

## 3 正则表达式 re

1.使用compile（）方法

~~~python
import re

pat = re.compile("AA")
res = pat.search("ABCAAFAA")
print(res) # <_sre.SRE_Match object; span=(3, 5), match='AA'> 匹配了第一个AA
~~~

2.使用search()与findall()方法

~~~python
import re

res = re.search("\d{2}","dj193") # 规则与待匹配的字符串，根据规则匹配字符(数量默认为1)，匹配成功则封装为一个对象返回
print(res)# <_sre.SRE_Match object; span=(2, 4), match='19'>

res1 = re.findall("\d","dj1234dd")# 根据规则匹配每一个字符，匹配成功的就存放到list中
print(res1) # ['1', '2', '3', '4']
~~~

3.sub()方法

~~~python
res2 = re.sub("a","1","abcdaat")# 在第三个字符串中找到第一个字符串并且使用第二个字符串进行替换
print(res2)# 1bcd11t
~~~



例子：

~~~python
# 正则表达式<a href="(.*?)/">中的.*?表示匹配任意字符（.），重复零次或多次（*）
# 但尽可能少地匹配（非贪婪匹配，?）。括号（）表示提取匹配的部分。
findLink = re.compile(r'<a href="(.*?)/">')
~~~

