# Mysql(八股学习型)



## 2级 数据库的三大范式

### **第一范式1NF**

确保数据库表字段的原子性。

比如字段 `userInfo`: `广东省 10086'` ，依照第一范式必须拆分成 `userInfo`: `广东省` `userTel`:` 10086`两个字段。

### **第二范式2NF**

首先要满足第一范式，另外包含两部分内容，一是表必须有一个主键；二是非主键列必须完全依赖于主键，而不能只依赖于主键的一部分。

举个例子。假定选课关系表为`student_course`(student_no, student_name, age, course_name, grade, credit)，主键为(student_no, course_name)。其中学分完全依赖于课程名称，姓名年龄完全依赖学号，不符合第二范式，会导致数据冗余（学生选n门课，姓名年龄有n条记录）、插入异常（插入一门新课，因为没有学号，无法保存新课记录）等问题。

应该拆分成三个表：学生：`student`(stuent_no, student_name, 年龄)；课程：`course`(course_name, credit)；选课关系：`student_course_relation`(student_no, course_name, grade)。

### **第三范式3NF**

首先要满足第二范式，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。

假定学生关系表为Student(student_no, student_name, age, academy_id, academy_telephone)，主键为"学号"，其中学院id依赖于学号，而学院地点和学院电话依赖于学院id，存在传递依赖，不符合第三范式。

可以把学生关系表分为如下两个表：学生：(student_no, student_name, age, academy_id)；学院：(academy_id, academy_telephone)。

### **2NF和3NF的区别？**

- 2NF依据是非主键列是否完全依赖于主键，还是依赖于主键的一部分。
- 3NF依据是非主键列是直接依赖于主键，还是直接依赖于非主键

# sql语句的执行流程

## 2级 一条查询sql语句的执行流程

### 学习版：



![](img/查询sql的执行过程.png)

MySQL 的逻辑架构图

大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。

Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

也就是说，你执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同。

从图中不难看出，不同的存储引擎共用一个**Server 层**，也就是从连接器到执行器的部分。



#### 4级 连接器

连接器负责跟客户端建立连接、获取权限、维持和管理连接。



### 回答：

查询语句的执行流程如下：权限校验(连接器)--->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎

当执行一条**查询sql**时，首先会去查询当前用户是否具有相应的权限，没有权限则会执行失败。如果有权限的话，在Mysql8.0版本之前缓存还没有被删除，则会去校验这个 SQL 是否执行过，是否以 Key-Value 的形式缓存在内存中,在缓存中能够命中的话就直接返回结果，如果没有命中缓存或者使用的是Mysql8.0之后的版本，则会走分析器来对sql进行关键字提取、语法分析，确认sql的语法无误后就会交给优化器来挑选Mysql认为最优的执行方案，当选择了执行方案后，接下来就会交给执行器准备开始执行了，执行之前还会进行一次权限验证，判断当前用户是否有执行的权限，如果没有权限，就会返回错误信息，如果有权限，才会调用存储引擎的接口来执行sql，并且返回接口执行的结果，如果有缓存的话还会把查询结果放到缓存中。

## 一条更新sql语句执行流程

### 学习版

#### update 语句执行流程

从一个表的一条更新语句举例，下面是这个表的创建语句，这个表有一个主键 ID 和一个整型字段 c：

```sql
mysql> create table T(ID int primary key, c int);
```

如果要将 ID=2 这一行的值加 1，SQL 语句就会这么写：

```r
mysql> update T set c=c+1 where ID=2;
```

**查询语句的那一套流程，更新语句也是同样会走一遍：**

你执行语句前要先连接数据库，这是连接器的工作。

接下来，**分析器会通过词法和语法解析知道这是一条更新语句**。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）。

另外，在一个表上有更新的时候，跟这个表有关的**查询缓存会失效**，所以这条语句就会把表 T 上所有缓存结果都清空。

我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。

![img](img/更新SQL执行的流程图.png)



你可能注意到了，最后三步看上去有点“绕”，将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是”两阶段提交”。

#### 为什么必须有“两阶段提交”呢？

两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。

这是为了让两份日志之间的逻辑一致。

如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。



#### redo log(重做日志)是什么？

##### 5级 为什么需要redo log

在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题, MySQL 里使用了 **WAL 技术**，WAL 的全称是 Write-Ahead Logging，它的**关键点就是先写日志，再写磁盘。**



1.  **当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存**，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。
2.  有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**。

##### redo log详解

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。

![img](img/redo log.png)

**write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。**

write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

#### binlog（归档日志）

 **redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。**

##### 为什么会有两份日志呢？

因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。

##### **两种日志的比较**

1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。





### 回答

更新语句执行流程如下：连接器(权限校验)----->分析器---->执行器根据索引调用存储引擎的接口寻找数据--->存储引擎找到数据后从磁盘写入内存中--->执行器从内存中获取数据进行更新操作后再调用引擎接口写入这行新数据---->引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面---->记录redo log(prepare 状态)--->执行器生成binlog--->redo log(commit 状态)

你执行语句前要先连接数据库，这是连接器的工作。

接下来，**分析器会通过词法和语法解析知道这是一条更新语句**。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）。

另外，在一个表上有更新的时候，跟这个表有关的**查询缓存会失效**，所以这条语句就会把表 T 上所有缓存结果都清空。

我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

# 事务

## 事务是什么

简单来说，**事务就是要保证一组数据库操作，要么全部成功，要么全部失败。**



## 事务四大特性ACID(三更故事记忆法)

 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性）

- 隔离性
  多个事务之间要相互隔离，不能互相干扰

- 原子性
  指事务是一个不可分割的整体，类似一个不可分割的原子

- 持久性
  指事务一旦被提交，这组操作修改的数据就真的的发生变化了(不会再被回滚了)。即使接下来数据库故障也不应该对其有影响。

- 一致性
  保障事务前后这组数据的状态是一致的。要么都是成功的，要么都是失败的。



## 事务隔离级别

事务隔离级别 transaction isolation level

**SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。**下面我逐一为你解释：

- 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。
- 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务**在启动时看到的数据是一致的**。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化，顾名思义是对于同一行记录，**“写”会加“写锁”，“读”会加“读锁”。**当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

### 3种错误现象



#### **脏读**

脏指：**读取到其它事务未提交的数据**

| **tx1**                                                    | **tx2**                                               |
| ---------------------------------------------------------- | ----------------------------------------------------- |
| set  session transaction isolation level read uncommitted; |                                                       |
| start  transaction;                                        |                                                       |
| select  * from account;  /*两个账户都为 1000*/             |                                                       |
|                                                            | start  transaction;                                   |
|                                                            | update  account set balance = 2000 where accountNo=1; |
| select  * from account; /*1号账户2000, 2号账户1000*/       |                                                       |

* tx2 未提交的情况下，tx1 仍然读取到了它的更改



#### **不可重复读**

在同一事务内，两次读取的结果不一致（第二次读取到了另一个事务修改、提交后的数据）

| **tx1**                                                  | **tx2**                                               |
| -------------------------------------------------------- | ----------------------------------------------------- |
| set  session transaction isolation level read committed; |                                                       |
| start  transaction;                                      |                                                       |
| select  * from account; /*两个账户都为 1000*/            |                                                       |
|                                                          | update  account set balance = 2000 where accountNo=1; |
| select  * from account; /1号账户2000/                    |                                                       |

* tx1 在同一事务内，两次读取的结果不一致



#### **幻读**

| **tx1**                                                      | **tx2**                               |
| ------------------------------------------------------------ | ------------------------------------- |
| set  session transaction isolation level repeatable read;    |                                       |
| start  transaction;                                          |                                       |
| select  * from account; /*存在 1,2 两个账户*/                |                                       |
|                                                              | insert  into account values(3, 1000); |
| select  * from account; /*发现还是只有 1,2 两个账户*/        |                                       |
| insert  into account values(3, 5000);  /* ERROR  1062 (23000): Duplicate entry '3' for key 'PRIMARY'  */ |                                       |

* tx1 查询时并没有发现 3 号账户，执行插入时却发现主键冲突异常，就好像出现了幻觉一样

### 4种隔离级别

1. Isolation.READ_UNCOMMITTED 读未提交,存在脏读、不可重复读、幻读问题

2. Isolation.READ_COMMITTED 读已提交,解决脏读问题，仍然存在不可重复读、幻读问题

3. Isolation.REPEATABLE_READ 可重复读,解决不可重复读问题，仍然存在幻读问题

4. Isolation.SERIALIZABLE 序列化,可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。

大多数数据库默认的事务隔离级别是Read committed，比如Sql Server , Oracle。

而Mysql的默认隔离级别是Repeatable read。

#### **读未提交**

* 读到其它事务未提交的数据（最新的版本）

* 错误现象：有脏读、不可重复读、幻读现象



#### **读已提交（RC）**

* 读到其它事务已提交的数据（最新已提交的版本）

* 错误现象：有不可重复读、幻读现象

* 使用场景：希望看到最新的有效值





#### **可重复读（RR）** 

* 在事务范围内，多次读能够保证一致性（快照建立时最新已提交版本）

* 错误现象：有幻读现象，可以用加锁避免

* 使用场景：事务内要求更强的一致性，但看到的未必是最新的有效值

原理：在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，**这个视图是在事务启动时创建的，整个事务存在期间都用这个视图**，所以即使真正的表被另一个事务改变了，由于读的是这个视图，仍然能够保证一致性。

在“读提交”隔离级别下，这个视图是在**每个 SQL 语句开始执行的时候**根据真正的表来创建的，所以其他事务把表进行修改提交后，能够查询得到。

这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，**没有视图概念**；

而“串行化”隔离级别下直接**用加锁的方式来避免并行访问**。

**加锁避免幻读**

| **tx1**                                                   | **tx2**                                           |
| --------------------------------------------------------- | ------------------------------------------------- |
| set  session transaction isolation level repeatable read; |                                                   |
| start  transaction;                                       |                                                   |
| select  * from account; /*存在 1,2 两个账户*/             |                                                   |
| select  * from account where accountNo=3  for update;     |                                                   |
|                                                           | insert  into account values(3, 1000);  /* 阻塞 */ |
| insert  into account values(3, 5000);                     |                                                   |

* 在 for update 这行语句执行时，虽然此时 3 号账户尚不存在，但 MySQL 在 repeatable read 隔离级别下会用**间隙锁**，锁住 2 号记录与正无穷大之间的间隙
* 此时 tx2 想插入 3 号记录就不行了，被间隙锁挡住了



#### **序列化(串行读)** 

* 在事务范围内，仅有读读可以并发，读写或写写会阻塞其它事务，用这种办法保证更强的一致性

* 错误现象：无



**串行读避免幻读**

| **tx1**                                                | **tx2**                                           |
| ------------------------------------------------------ | ------------------------------------------------- |
| set  session transaction isolation level serializable; |                                                   |
| start  transaction;                                    |                                                   |
| select  * from account; /* 存在 1,2 两个账户 */        |                                                   |
|                                                        | insert  into account values(3, 1000);  /* 阻塞 */ |
| insert  into account values(3, 5000);                  |                                                   |

* 串行读隔离级别下，普通的 select 也会加**共享读锁**，其它事务的查询可以并发，但增删改就只能阻塞了



## 事务传播行为propagation

当**事务方法嵌套调用时**，需要控制是否开启新事务，可以使用事务传播行为来控制。

| 属性值                                                   | 行为                                                   |
| -------------------------------------------------------- | ------------------------------------------------------ |
| REQUIRED（必须要有：别人有我就进入别人的，没有就新建）   | 外层方法有事务，内层方法就加入。外层没有，内层就新建   |
| REQUIRES_NEW（必须要有新事务：不管别人有没有，我都新建） | 外层方法有事务，内层方法新建。外层没有，内层也新建     |
| SUPPORTS（支持有：别人有我就进入别人的，没有就摆了）     | 外层方法有事务，内层方法就加入。外层没有，内层就也没有 |
| NOT_SUPPORTED（支持没有:我谁也不加）                     | 外层方法有事务，内层方法没有。外层没有，内层也没有     |
| MANDATORY（强制要求外层有）                              | 外层方法有事务，内层方法加入。外层没有。内层就报错     |
| NEVER(绝不允许有)                                        | 外层方法有事务，内层方法就报错。外层没有。内层就也没有 |

**重点掌握：前三个即可**。

**(内层)事务为什么失效？**可能是事务传播行为propagation设置错误，比如设置为SUPPORTS并且此时外层方法没有设置事务时，内层的事务就会失效。

# 存储引擎

##  InnoDB vs MyISAM

### **要求**

* 掌握 InnoDB 与 MyISAM 的主要区别
* 尤其注意它们在索引结构上的区别

### **InnoDB**

* 索引分为聚簇索引与二级索引
  * 聚簇索引：主键值作为索引数据，叶子节点还包含了所有字段数据，索引和数据是存储在一起的
  * 二级索引：除主键外的其它字段建立的索引称为二级索引。被索引的字段值作为索引数据，叶子节点还包含了主键值

* 支持事务
  * 通过 undo log 支持事务回滚、当前读（多版本查询）
  * 通过 redo log 实现持久性
  * 通过两阶段提交实现一致性
  * 通过当前读、锁实现隔离性

* 支持行锁、间隙锁

* 支持外键

### **MyISAM**

* 索引只有一种
  * 被索引字段值作为索引数据，叶子节点还包含了该记录数据页地址，数据和索引是分开存储的
* 不支持事务，没有 undo log 和 redo log

* 仅支持表锁

* 不支持外键

* 会保存表的总行数

### **InnoDB 索引特点**

#### 聚簇索引：

主键值作为索引数据，叶子节点还包含了所有字段数据，索引和数据是存储在一起的

![image-20210901155308778](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901155308778.png)

* 主键即 7369、7499、7521 等

#### 二级索引：

除主键外的其它字段建立的索引称为二级索引。被索引的字段值作为索引数据，叶子节点还包含了主键值

![image-20210901155317460](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901155317460.png)

* 上图中 800、950、1100 这些是工资字段的值，根据它们建立了二级索引

![image-20210901155327838](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901155327838.png)

* 上图中，如果执行查询 `select empno, ename, sal from emp where sal = 800`，这时候可以利用二级索引定位到 800 这个工资，同时还能知道主键值 7369
* 但 select 字句中还出现了 ename 字段，在二级索引中不存在，因此需要根据主键值 7369 查询聚簇索引来获取 ename 的信息，这个过程俗称**回表**



### **MyISAM 索引特点**

被索引字段值作为索引数据，叶子节点还包含了该记录数据页地址，数据和索引是分开存储的

![image-20210901155226621](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901155226621.png)



# 索引

##  索引

### **要求**

* 了解常见索引与它们的适用场景，尤其是 B+Tree 索引的特点
* 掌握索引用于排序，以及失效情况
* 掌握索引用于筛选，以及失效情况
* 理解索引条件下推
* 理解二级索引覆盖

### 索引基础

#### 4级 **常见索引**的实现

* 哈希索引
  * 理想时间复杂度为 $O(1)$
  * 适用场景：适用于等值查询的场景，内存数据的索引
  * 典型实现：Redis，MySQL 的 memory 引擎
* 平衡二叉树索引 
  * 查询和更新的时间复杂度都是 $O(log_2(n))$
  * 适用场景：适用于等值查询以及范围查询；适合内存数据的索引，但不适合磁盘数据的索引，可以认为**树的高度决定了磁盘 I/O 的次数**，百万数据树高约为 20
* BTree 索引
  * BTree 其实就是 n 叉树，分叉多意味着节点中的孩子（key）多，树高自然就降低了
  * 分叉数由页大小和行（包括 key 与 value）大小决定
    * 假设页大小为 16k，每行 40 个字节，那么分叉数就为 16k / 40 ≈ 410
    * 而分叉为 410，则百万数据树高约为3，仅 3 次 I/O 就能找到所需数据
  * **局部性原理**：每次 I/O 按页为单位读取数据，把多个 **key 相邻**的行放在同一页中（每页就是树上一个节点），能进一步减少 I/O

* B+ 树索引 
  * 在 BTree 的基础上做了改进，索引上只存储 key，这样能进一步增加分叉数，假设 key 占 13 个字节，那么一页数据分叉数可以到 1260，树高可以进一步下降为 2

> ***树高计算公式***
>
> * $log_{10}(N) /  log_{10}(M)$ 其中 N 为数据行数，M 为分叉数



#### **BTree vs B+Tree**

* 无论 BTree 还是 B+Tree，每个叶子节点到根节点距离都相同
* **BTree key 及 value 存储在每个节点上，无论叶子还是非叶子节点**

<img src="D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901170943656.png" alt="image-20210901170943656" style="zoom:80%;" />

* **B+Tree 普通节点只存 key，叶子节点才存储 key 和 value**，因此分叉数可以更多
  * 不过也请注意，普通节点上的 key 有的会与叶子节点的 key 重复
* B+Tree 必须到达叶子节点才能找到 value
* B+Tree 叶子节点用链表连接，可以方便范围查询及全表遍历

<img src="D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901170954328.png" alt="image-20210901170954328" style="zoom:80%;" />

> 注：这两张图都是仅画了 key，未画 value



#### **B+Tree 新增 key的过程**

假设阶数（m）为5

1. 若为空树，那么直接创建一个节点，插入 key 即可，此时这个叶子结点也是根结点。例如，插入 5

   ![image-20210901174939408](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901174939408.png)

2. 插入时，若当前结点 key 的个数小于阶数，则插入结束

3. 依次插入 8、10、15，按 key 大小升序

   ![image-20210901175021697](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175021697.png)

4. 插入 16，这时到达了阶数限制，所以要进行分裂

   ![image-20210901175057315](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175057315.png)

5. **叶子节点分裂规则**：将这个叶子结点分裂成左右两个叶子结点，左叶子结点包含前 m/2 个（2个）记录，右结点包含剩下的记录，将中间的 key 进位到父结点中。**注意**：中间的 key 仍会保留在叶子节点一份

   ![image-20210901175106713](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175106713.png)

6. 插入 17

   ![image-20210901175333804](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175333804.png)

7. 插入 18，这时当前结点的 key 个数到达 5，进行分裂

   ![image-20210901175352807](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175352807.png)

8. 分裂成两个结点，左结点 2 个记录，右结点 3 个记录，key 16 进位到父结点中

   ![image-20210901175413305](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175413305.png)

9. 插入 19、20、21、22、6、9

   ![image-20210901175440205](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175440205.png)

10. 插入 7，当前结点的 key 个数到达 5，需要分裂

    ![image-20210901175518737](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175518737-16304901199481.png)

11. 分裂后 key 7 进入到父结点中，这时父节点 key 个数也到达 5

    ![image-20210901175544893](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175544893.png)

12. **非叶子节点分裂规则**：左子结点包含前 (m-1)/2 个 key，将中间的 key 进位到父结点中（**不保留**），右子节点包含剩余的 key

    ![image-20210901175617464](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175617464.png)



#### **B+Tree 查询 key的过程**

以查询 15 为例

* 第一次 I/O

  ![image-20210901175721826](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175721826.png)

* 第二次 I/O

  ![image-20210901175738876](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175738876-16304902605912.png)

* 第三次 I/O

  ![image-20210901175801859](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901175801859.png)



**B+Tree 删除叶子节点 key**

1. 初始状态

   ![image-20210901180320860](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901180320860.png)

2. **删完有富余**。即删除后结点的key的个数 > m/2 – 1，删除操作结束，例如删除 22

   ![image-20210901180331158](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901180331158.png)

3. **删完没富余，但兄弟节点有富余**。即兄弟结点 key 有富余（ > m/2 – 1 ），向兄弟结点借一个记录，同时替换父节点，例如删除 15

   ![image-20210901180356515](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901180356515.png)

4. **兄弟节点也不富余，合并兄弟叶子节点**。即兄弟节点合并成一个新的叶子结点，并删除父结点中的key，将当前结点指向父结点，例如删除 7

   ![image-20210901180405393](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901180405393.png)

5. 也需要删除非叶子节点中的 7，并替换父节点保证区间仍有效

   ![image-20210901180422491](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901180422491.png)

6. 左右兄弟都不够借，合并

   ![image-20210901180446827](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901180446827.png)



#### **B+Tree 删除非叶子节点 key的过程**

接着上面的操作

1. 非叶子节点 key 的个数 > m/2 – 1，则删除操作结束，否则执行 2

2. 若**兄弟结点有富余**，父结点 key 下移，兄弟结点 key 上移，删除结束，否则执行 3

3. 若**兄弟节点没富余**，当前结点和兄弟结点及父结点合并成一个新的结点。重复 1

   ![image-20210901180511685](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901180511685.png)

   ![image-20210901180516139](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210901180516139.png)



### 命中索引

> **准备数据**
>
> 1. 修改 MySQL 配置文件，在 [mysqld] 下添加 secure_file_priv= 重启 MySQL 服务器，让选项生效
>
> 2. 执行 db.sql 内的脚本，建表
>
> 3. 执行 `LOAD DATA INFILE 'D:\\big_person.txt' INTO TABLE big_person;` 注意实际路径根据情况修改
>    * 测试表 big_person（此表数据量较大，如果与其它表数据一起提供不好管理，故单独提供），数据行数 100 万条，列个数 15 列。为了更快速导入数据，这里采用了 load data infile 命令配合 *.txt 格式数据

#### **索引用于排序**

##### 多列排序时使用索引

结论：

单列索引并不能在多列排序时被使用，多列排序想要走索引的话需要建立**组合索引**(遵循最左前缀原则)并且多列排序**每个字段的升降序需要一致**。

例子：

```sql
/* 测试单列索引并不能在多列排序时加速 */
create index first_idx on big_person(first_name);
create index last_idx on big_person(last_name);
explain select * from big_person order by last_name, first_name limit 10; 

/* 多列排序需要用组合索引 */
alter table big_person drop index first_idx;
alter table big_person drop index last_idx;
create index last_first_idx on big_person(last_name,first_name);

/* 多列排序需要遵循最左前缀原则, 第1个查询可以利用索引，第2,3查询不能利用索引 */
explain select * from big_person order by last_name, first_name limit 10; 
explain select * from big_person order by first_name, last_name limit 10; 
explain select * from big_person order by first_name limit 10; 

/* 多列排序升降序需要一致，查询1可以利用索引，查询2不能利用索引*/
explain select * from big_person order by last_name desc, first_name desc limit 10; 
explain select * from big_person order by last_name desc, first_name asc limit 10; 
```



 ##### **order关键字的最左前缀原则**

排序字段中的第一个字段必须**以组合索引的第一个字段开头**，并且按照组合索引的顺序来看是**连续的**才能够使用索引。

>
> 若建立组合索引 (a,b,c)，则可以**利用**到索引的**排序**条件是：
>
> * order by a
> * order by a, b
> * order by a, b, c



#### 4级 **索引用于 where 筛选**

* 参考 https://dev.mysql.com/doc/refman/8.0/en/multiple-column-indexes.html

##### 索引失效

1. 函数及计算问题，一旦在字段上应用了计算或函数，都会造成索引失效。
2. 发生隐式类型转换等价于在某个字段上应用了函数，造成索引失效

##### where使用组合索引的条件

结论：

1. where + 模糊查询 想要走组合索引时，需要遵循字符串最左前缀原则
2. where + and 想要走组合索引**只要where字段中出现组合索引的第一个字段即可，不要求顺序，特殊情况：索引下推**

例子：

```sql
/* 模糊查询需要遵循字符串最左前缀原则，查询2可以利用索引，因为建立的组合索引为(last_name,first_name)，查询1,3不能利用索引 */
explain SELECT * FROM big_person WHERE first_name LIKE 'dav%' LIMIT 5;
explain SELECT * FROM big_person WHERE last_name LIKE 'dav%' LIMIT 5;
explain SELECT * FROM big_person WHERE last_name LIKE '%dav' LIMIT 5;

/* 组合索引需要遵循最左前缀原则，查询1,2可以利用索引，查询3,4不能利用索引 */
create index province_city_county_idx on big_person(province,city,county);
explain SELECT * FROM big_person WHERE province = '上海' AND city='宜兰县' AND county='中西区';
explain SELECT * FROM big_person WHERE county='中西区' AND city='宜兰县' AND province = '上海'; // mark1 question
explain SELECT * FROM big_person WHERE city='宜兰县' AND county='中西区';
explain SELECT * FROM big_person WHERE county='中西区';

/* 函数及计算问题，一旦在字段上应用了计算或函数，都会造成索引失效。查询2可以利用索引，查询1不能利用索引 */
create index birthday_idx on big_person(birthday);
explain SELECT * FROM big_person WHERE ADDDATE(birthday,1)='2005-02-10';
explain SELECT * FROM big_person WHERE birthday=ADDDATE('2005-02-10',-1);

/* 隐式类型转换问题
	* 查询1会发生隐式类型转换等价于在phone字段上应用了函数，造成索引失效
	* 查询2字段与值类型相同不会类型转换，可以利用索引
*/
create index phone_idx on big_person(phone);
explain SELECT * FROM big_person WHERE phone = 13000013934;
explain SELECT * FROM big_person WHERE phone = '13000013934';
```

> ***最左前缀原则（leftmost prefix）***
>
> 若建立组合索引 (a,b,c)，则可以**利用**到索引的查询条件是：
>
> * where a = ?
> * where a = ? and b = ? （注意与条件的先后次序无关，也可以是 where b = ? and a = ?，只要出现即可）
> * where a = ? and b = ? and c = ? （注意事项同上）
>
> **不能利用**的例子：
>
> * where b = ?
> * where b = ? and c = ?
> * where c = ?
>
> **即：想要走组合索引，只要字段中出现组合索引的第一个字段即可**
>
> 特殊情况：
>
> * where a = ? and c = ?（a = ? 会利用索引，但 c = ? 不能利用索引加速，会触发索引条件下推）



##### **索引条件下推**

* 参考 https://dev.mysql.com/doc/refman/8.0/en/index-condition-pushdown-optimization.html

```sql
/* 查询 1,2,3,4 都能利用索引，但 4 相当于部分利用了索引，会触发索引条件下推 */
explain SELECT * FROM big_person WHERE province = '上海';
explain SELECT * FROM big_person WHERE province = '上海' AND city='嘉兴市';
explain SELECT * FROM big_person WHERE province = '上海' AND city='嘉兴市' AND county='中西区';
explain SELECT * FROM big_person WHERE province = '上海' AND county='中西区';
```

> ***索引条件下推***
>
> * MySQL 执行条件判断的时机有两处：
>   * 服务层（上层，不包括索引实现）
>   * 引擎层（下层，包括了索引实现，可以利用）
>   * 上面查询 4 中有 province 条件能够利用索引，在引擎层执行，但 county 条件仍然要交给服务层处理
> * 在 5.6 之前，服务层需要判断所有记录的 county 条件，性能非常低
> * 5.6 以后，引擎层会先根据 province 条件过滤，满足条件的记录才在服务层处理 county 条件

我们现在用的是 5.6 以上版本，所以没有体会，可以用下面的语句关闭索引下推优化，再测试一下性能

```sql
SET optimizer_switch = 'index_condition_pushdown=off';
SELECT * FROM big_person WHERE province = '上海' AND county='中西区';
```



##### **二级索引覆盖**

```sql
explain SELECT * FROM big_person WHERE province = '上海' AND city='宜兰县' AND county= '中西区';
explain SELECT id,province,city,county FROM big_person WHERE province = '上海' AND city='宜兰县' AND county='中西区';
```

根据查询条件查询 1，2 都会先走二级索引，但是二级索引仅包含了 (province, city, county) 和 id 信息

* 查询 1 是 select *，因此还有一些字段二级索引中没有，需要回表（查询聚簇索引）来获取其它字段信息
* 查询 2 的 select 中明确指出了需要哪些字段，这些字段在二级索引都有，就避免了回表查询



**其它注意事项**

* 表连接需要在连接字段上建立索引
* 不要迷信网上说法，具体情况具体分析

例如：

```sql
create index first_idx on big_person(first_name);

/* 不会利用索引，因为优化器发现查询记录数太多，还不如直接全表扫描 */
explain SELECT * FROM big_person WHERE first_name > 'Jenni';

/* 会利用索引，因为优化器发现查询记录数不太多 */
explain SELECT * FROM big_person WHERE first_name > 'Willia';

/* 同一字段的不同值利用 or 连接，会利用索引 */
explain select * from big_person where id = 1 or id = 190839;

/* 不同字段利用 or 连接，会利用索引(底层分别用了两个索引) */
explain select * from big_person where first_name = 'David' or last_name = 'Thomas';

/* in 会利用索引 */
explain select * from big_person where first_name in ('Mark', 'Kevin','David'); 

/* not in 不会利用索引的情况 */
explain select * from big_person where first_name not in ('Mark', 'Kevin','David');

/* not in 会利用索引的情况 */
explain select id from big_person where first_name not in ('Mark', 'Kevin','David');
```

* 以上实验基于 5.7.27，其它如 !=、is null、is not null 是否使用索引都会跟版本、实际数据相关，以优化器结果为准





## InnoDB 的索引模型

在 InnoDB 中，**表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。**

每一个索引在 InnoDB 里面对应一棵 B+ 树。

假设，我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。

这个表的建表语句是：

```sql
mysql> create table T(
id int primary key, 
k int not null, 
name varchar(16),
index (k))engine=InnoDB;
```

表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。

**树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。**

![img](img/索引的数据结构.png)

图 4 InnoDB 的索引组织结构

### 主键索引和非主键索引

从图中不难看出，根据叶子节点的内容，**索引类型分为主键索引和非主键索引**。

主键索引的叶子节点存的是**整行数据**。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。

非主键索引的叶子节点内容是**主键的值**。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

根据上面的索引结构说明，我们来讨论一个问题：**基于主键索引和普通索引的查询有什么区别？**

- 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；
- 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

### 覆盖索引

如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，**在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。**

**由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。**

### 联合索引

**在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？**

假设这个市民表的定义是这样的：

```r
CREATE TABLE `tuser` (
  `id` int(11) NOT NULL,
  `id_card` varchar(32) DEFAULT NULL,
  `name` varchar(32) DEFAULT NULL,
  `age` int(11) DEFAULT NULL,
  `ismale` tinyint(1) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `id_card` (`id_card`),
  KEY `name_age` (`name`,`age`)
) ENGINE=InnoDB
```

我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求(会走身份证号的索引)，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？

如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。

当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。

### 最左前缀原则

看到这里你一定有一个疑问，如果为每一种查询都设计一个索引，索引是不是太多了。如果我现在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？

这里，我先和你说结论吧。**B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。** mark1 question

### 索引下推

上一段我们说到**满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。**这时，你可能要问，那些不符合最左前缀的部分，会怎么样呢？

我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的：

```sql
mysql> select * from tuser where name like '张 %' and age=10 and ismale=1;
```

你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好。

然后呢？

当然是判断其他条件是否满足。

在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。

而 MySQL 5.6 引入的**索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。**

图 3 和图 4，是这两个过程的执行流程图。

![img](https://lianglianglee.com/%e6%9e%81%e5%ae%a2%e6%97%b6%e9%97%b4/assets/b32aa8b1f75611e0759e52f5915539ac-1584367388568.jpg)

图 3 无索引下推执行流程

![img](https://lianglianglee.com/%e6%9e%81%e5%ae%a2%e6%97%b6%e9%97%b4/assets/76e385f3df5a694cc4238c7b65acfe1b-1584367388573.jpg)

图 4 索引下推执行流程

在图 3 和 4 这两个图里面，每一个虚线箭头表示回表一次。

图 3 中，在 (name,age) 索引里面我特意去掉了 age 的值，这个过程 InnoDB 并不会去看 age 的值，只是按顺序把“name 第一个字是’张’”的记录一条条取出来回表。因此，需要回表 4 次。

图 4 跟图 3 的区别是，InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。在我们的这个例子中，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。



# 锁

## 锁

### **要求**

* 了解全局锁
* 了解表级锁
* 掌握行级锁

**数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。**

**根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类**。

### 全局锁

**全局锁就是对整个数据库实例加锁。**

用作全量备份时，保证**表与表之间的数据一致性**

如果不加任何锁，数据备份时就可能产生不一致的情况，如下图所示

![image-20210902090302805](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210902090302805.png)

全局读锁的语法：

```sql
flush tables with read lock;	
```

* 使用全局读锁锁定所有数据库的所有表。**让整个库处于只读状态**这时会阻塞其它线程所有 DML 以及 DDL 操作，这样可以避免备份过程中的数据不一致。接下来可以执行备份，最后用 unlock tables 来解锁

> ***注意***
>
> 但 flush tables 属于比较重的操作，可以使用 --single-transaction 参数来完成不加锁的一致性备份（仅针对 InnoDB 引擎的表）
>
> ```sql
> mysqldump --single-transaction -uroot -p test > 1.sql
> ```

### 表级锁

#### **表级锁 - 表锁**

* 语法：加锁 lock tables 表名 read/write，解锁 unlock tables
* 缺点：粒度较粗，在 InnoDB 引擎很少使用



#### **表级锁 - 元数据锁**

* 即 metadata-lock（MDL），主要是为**了避免 DML 与 DDL 冲突**，DML 的元数据锁之间不互斥

* 加元数据锁的几种情况
  * `lock tables read/write`，类型为 SHARED_READ_ONLY 和 SHARED_NO_READ_WRITE
  * `alter table`，类型为 EXCLUSIVE，与其它 MDL 都互斥
  * `select，select … lock in share mode`，类型为 SHARED_READ
  * `insert，update，delete，select for update`，类型为 SHARED_WRITE 

* 查看元数据锁（适用于 MySQL 8.0 以上版本）
  * `select object_type,object_schema,object_name,lock_type,lock_duration from performance_schema.metadata_locks;`

 **MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。**



在 MySQL 5.5 版本中引入了 MDL，**当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。**

- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 mark2 question
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

**事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。**



### 3级 **行级锁**

MySQL 的行锁是在引擎层由各个引擎自己实现的。但**并不是所有的引擎都支持行锁**，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

**在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。**



* 种类
  * 行锁(记录锁) – 在 RC 下，锁住的是行，防止其他事务对此行 update 或 delete
  * 间隙锁 – 在 RR 下，锁住的是间隙，防止其他事务在这个间隙 insert 产生幻读
  * 临键锁 – 在 RR 下，锁住的是前面间隙+行，特定条件下可优化为行锁(记录锁)

* 查看行级锁
  * `select object_schema,object_name,index_name,lock_type,lock_mode,lock_data from performance_schema.data_locks where object_name='表名';`

> ***注意***
>
> * 它们锁定的其实都是**索引**上的行与间隙，根据索引的有序性来确定间隙



测试数据

```sql
create table t (id int primary key, name varchar(10),age int, key (name)); 
insert into t values(1, 'zhangsan',18); 
insert into t values(2, 'lisi',20); 
insert into t values(3, 'wangwu',21); 
insert into t values(4, 'zhangsan', 17); 
insert into t values(8,'zhang',18);
insert into t values(12,'zhang',20);
```

> **说明**
>
> * 1,2,3,4 之间其实并不可能有间隙
> * 4 与 8 之间有间隙
> * 8 与 12 之间有间隙
> * 12 与正无穷大之间有间隙
> * 其实我们的例子中还有负无穷大与 1 之间的间隙，想避免负数可以通过建表时选择数据类型为 unsigned int



间隙锁例子

事务1：

```sql
begin;
select * from t where id = 9 for update; /* 锁住的是 8 与 12 之间的间隙 */
```

事务2：mark1 question

```sql
update t set age=100 where id = 8; /* 不会阻塞 */
update t set age=100 where id = 12; /* 不会阻塞 */
insert into t values(10,'aaa',18); /* 会阻塞 */ 
```



临键锁和记录锁例子

事务1：

```sql
begin;
select * from t where id >= 8 for update;
```

* 临键锁锁定的是**左开右闭**的区间，与上条查询条件相关的区间有 (4,8]，(8,12]，(12,+∞)
* 临键锁在某些条件下可以被优化为记录锁，例如 (4,8] 被优化为只针对 8 的记录锁，前面的区间不会锁住

事务2：

```sql
insert into t values(7,'aaa',18); /* 不会阻塞 */
update t set age=100 where id = 8; /* 会阻塞 */
insert into t values(10,'aaa',18); /* 会阻塞 */
update t set age=100 where id = 12; /* 会阻塞 */
insert into t values(13,'aaa',18); /* 会阻塞 */
```



### 死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。![img](img/死锁的形成.png)

这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。**当出现死锁以后，有两种策略**：

- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。

在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。

但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，**超时时间设置太短的话，会出现很多误伤。**

所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。**主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。**

你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

那如果是我们上面说到的所有事务都要更新同一行的场景呢？

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。

根据上面的分析，我们来讨论一下，**怎么解决由这种热点行更新导致的性能问题呢？**问题的症结在于，死锁检测要耗费大量的 CPU 资源。

**一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。**但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。

**另一个思路是控制并发度。**根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。

因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。

可能你会问，**如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？**

你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。

这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。

**小结：**

我以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句。这里的原则 / 我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。

但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事务量。

# log

## undo log 与 redo log

### **要求**

* 理解 undo log 的作用
* 理解 redo log 的作用

### **undo log**

* 回滚数据，以行为单位，记录数据每次的变更，一行记录有多个版本并存
* 多版本并发控制，即快照读（也称为一致性读），让查询操作可以去访问历史版本

![image-20210902083051903](D:\alwaysUse\notes\myNotes\java interview note\山哥面试专题\数据库\讲义\img\image-20210902083051903.png)

1. 每个事务会按照开始时间，分配一个单调递增的事务编号 trx id
2. 每次事务的改动都会以行为单位记入回滚日志，包括当时的事务编号，改动的值等
3. 查询操作，事务编号大于自己的数据是不可见的，事务编号小于等于自己的数据才是可见的
   * 例如图中红色事务看不到 trx id=102 以及 trx id=101 的数据，只有 trx id=99 的数据对它可见



### **redo log**

redo log 的作用主要是**实现 ACID 中的持久性**，保证提交的数据不丢失

* 它记录了事务提交的变更操作，服务器意外宕机重启时，利用 redo log 进行回放，重新执行已提交的变更操作
* 事务提交时，首先将变更写入 redo log，事务就视为成功。至于数据页（表、索引）上的变更，可以放在后面慢慢做
  * 数据页上的变更宕机丢失也没事，因为 redo log 里已经记录了
  * 数据页在磁盘上位置随机，写入速度慢，redo log 的写入是顺序的速度快

它**由两部分组成**，内存中的 redo log buffer，磁盘上的 redo log file

* redo log file 由一组文件组成，当写满了会循环覆盖较旧的日志，这意味着不能无限依赖 redo log，更早的数据恢复需要 binlog 
* buffer 和 file 两部分组成意味着，写入了文件才真正安全，同步策略由参数 innodb_flush_log_at_trx_commit  控制
  * 0 - 每隔 1s 将日志 write and flush 到磁盘 
  * 1 - 每次事务提交将日志 write and flush（默认值）
  * 2 - 每次事务提交将日志 write，每隔 1s flush 到磁盘，意味着 write 意味着写入操作系统缓存，如果 MySQL 挂了，而操作系统没挂，那么数据不会丢失







## 2级 (uodo log详解)“快照”在 MVCC 里是怎么工作的？

**在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。**

这时，你会说这看上去不太现实啊。如果一个库有 100G，那么我启动一个事务，MySQL 就要拷贝 100G 的数据出来，这个过程得多慢啊。可是，我平时的事务执行起来很快啊。

实际上，我们并不需要拷贝出这 100G 的数据。我们先来看看这个快照是怎么实现的。

**InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。**

**而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。**

**也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。**

如图 2 所示，就是一个记录被多个事务连续更新后的状态。

![img](https://lianglianglee.com/%e6%9e%81%e5%ae%a2%e6%97%b6%e9%97%b4/assets/68d08d277a6f7926a41cc5541d3dfced-1584367388595.png)

图 2 行状态变更图

图中虚线框里是同一行数据的 4 个版本，当前最新版本是 V4，k 的值是 22，它是被 transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。

你可能会问，前面的文章不是说，语句更新会生成 **undo log（回滚日志）**吗？那么，**undo log 在哪呢？**

实际上，图 2 中的三个虚线箭头，就是 undo log；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，**需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。**

明白了多版本和 row trx_id 的概念后，我们再来想一下，InnoDB 是怎么定义那个“100G”的快照的。

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。

因此，**一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据的（事务？）版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个（事务？）版本”。**

当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。

**my：简单来说：即：一个可重复读隔离级别的事务启动时建立的一致性快照（这整个事务过程中可见的数据）必须为：数据对应的事务id <= 当前这个事务的id的数据，对于数据的事务id大于当前事务id的数据，在当前这个事务看来是不可见的**